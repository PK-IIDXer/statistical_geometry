\chapter{確率分布がなす多様体}

皆さんは当然、\textbf{正規分布}、みたことあると思います。
\[
  p(x;\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}
\]
これは平均$\mu\in\mathbb{R}$と分散$\sigma>0$が定まれば、一つの正規分布が定まると見ることができます。
このように、\textbf{確率分布をパラメータづける空間}を幾何的に調べてみましょう、というのが素朴な出発点のようです。
確率分布$p(x)$が$\xi\in\mathbb{R}^n$でパラメータづけることができる場合、これを
\[
  p(x;\xi)
\]
と書きます。

もちろんこれだけでは、好き勝手に$\mathbb{R}^n$の開集合を考えてるだけにすぎないです。
ここに\textbf{Fisher情報量}あるいは\textbf{Fisher情報行列}と呼ばれる構造を含めて考えると、これがそのままRiemann計量になるという指摘が古くからなされています\cite{Rao45}。
ものとしては、エントロピー$\log p(x;\xi)$を$\xi^i$で微分したものの積の、期待値です。
\[
  g_{ij}(\xi):=E_\xi\left[\frac{\partial \log p(x;\xi)}{\partial \xi^i} \, \frac{\partial \log p(x;\xi)}{\partial \xi^j}\right]
\]
正規分布について上記のFisher情報量を考えると、双曲平面にそっくりな計量が現れ、負の定曲率の曲面となります。
甘利教授は、このことに美しさを感じたといいます\cite{Am25}。

Riemann幾何を学んだ人は、計量があればそれに付随するRiemann接続を考えたくなります。
これも好き勝手な接続を考えるのではなく、統計学的に自然な接続を考えることができます。
このような接続$\nabla^{(\alpha)}$ ($\alpha$は任意の実数)は、一般にはRiemann接続にはなっていません。
しかし一般にベクトル場$X,Y,Z$に対して
\[
  Xg(Y,Z)=g(\nabla_X^{(\alpha)}Y,Z)+g(Y,\nabla_X^{(-\alpha)}Z)
\]
を満たすという意味で、$\nabla^{(\alpha)}$と$\nabla^{(-\alpha)}$はFisher情報量に関して\textbf{双対接続}となっています。
また$\nabla^{(\alpha)}$の接続係数は、Riemann接続$\nabla=\nabla^{(0)}$の接続係数$\Gamma_{ij,k}^{(0)}$によって
\[
  \Gamma_{ij,k}^{(\alpha)}(\xi)=\Gamma_{ij,k}^{(0)}(\xi)+\frac{1-\alpha}{2}E_{\xi}\left[\frac{\partial \log p(x;\xi)}{\partial \xi^i} \, \frac{\partial \log p(x;\xi)}{\partial \xi^j} \, \frac{\partial \log p(x;\xi)}{\partial \xi^k}\right]
\]
とあらわされるものですから、これらをもとに微分幾何をやろうというのは非常に自然な発想と言えましょう。




\section{統計モデルとFisher情報量}

Fisher情報量とは一言でいうと「推定のしやすさをあらわす量」です。
つまり、Fisher情報量が大きければ大きいほど、少ないサンプルから妥当な推定ができると言え、Fisher情報量が小さければ小さいほど、妥当な推定を行うために多くのサンプルが必要となると言える量です。
例えば正規分布であれば、分散が小さければ小さいほど山が尖っており、少ないサンプルでもおおよそ「どこにピークがあるか？」を探しやすいです。
これはラジオのチューニングにたとえることができるでしょう\footnote{
  今はそもそもラジオを聞く人がいないかもしれませんが、昔はアナログなツマミを回して目当ての電波を拾っていました。
}。

\subsection{統計モデル}

Fisher情報量の前に、そもそも何を推定するのかというのを数学的にモデル化しましょう。
\begin{definition}[統計モデル]
  $(\Omega,\mathcal{B})$を測度空間とする。
  また、$n>0$を整数とし、$M$を$\mathbb{R}^n$の部分集合とする。
  $\Omega$上の確率密度の族$S$が$M$でパラメータ付けられる場合、即ち
  \[
    S=\{p_{\xi}=p(x;\xi)\mid \xi=(\xi^1,\dots,\xi^n)\in M\}
  \]
  と書けており、かつ対応
  \[
    M\to S;\xi\mapsto p_{\xi}
  \]
  が単射であるとき、$n$次元\textbf{統計モデル}、または、\textbf{パラメトリックモデル}、または単に\textbf{モデル}という。
\end{definition}
測度空間$(\Omega,\mathcal{B})$と、$n$個の実数でパラメトライズされた確率密度の族を統計モデルというわけです。
Bourbaki流に$(\Omega,\mathcal{B},M,S)$とでも書くべきでしょうか。
つまり、いくつかのサンプル$\{\xi_i\}_{i=1}^N$から、現実をよく表わしているはずの確率密度を探し当てようということです。

ところがこのままではパラメータで微分できない可能性があるため、微分幾何学で統計モデルを解析するという際に不都合がある場合があります。
微分可能な統計モデルに一旦適当に名前を付けておきます。
\begin{definition}
  統計モデル$(\Omega,\mathcal{B},M,S)$が\textbf{可微分統計モデル}であるとは、$M\subset\mathbb{R}^n$が開集合であり、各$x\in\Omega$に対して写像
  \[
    M\to\mathbb{R};\xi\mapsto p(x;\xi)
  \]
  が滑らかであるときをいう。
\end{definition}

\subsubsection{統計モデルの例}

正規分布が可微分統計モデルの定義をみたしていることを確認しましょう。
\begin{example}[正規分布]
  平均$\mu\in\mathbb{R}$、偏差$\sigma>0$の正規分布
  \[
    p(x;\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}
  \]
  の族は、明らかに2次元可微分統計モデルである。
\end{example}
非常に多くの身の回りの自然現象が正規分布に従っていると言えます。
例えば国民の身長・体重の分布や、テストの点数、工業製品の規格からのズレ具合、気体分子運動などなど。
十分多くのデータがあれば、その独立な特徴量は正規分布になるという中心極限定理も知られています。
もはや正規分布の重要性はいうに及ばないでしょう。

\begin{example}[Poisson分布]
  \[
    \Omega=\{0,1,2,\dots\},\quad n=1,\quad M=\{\xi\mid\xi>0\}
  \]
  に対して確率分布の族
  \[
    p(x;\xi)=e^{-\xi}\frac{\xi^x}{x!}
  \]
  は、1次元可微分統計モデルである。
\end{example}
Poisson分布は、放射性物質から放出される単位時間あたりの粒子の個数のような原子核論・素粒子論や、身近なところだとウェブアクセスの1分間あたりのPVなどに現れます。
概して「ある期間（空間）で、稀にしか起こらないが、ランダムかつ独立に発生する事象の回数」をモデル化することで現れてくるものです。

\begin{example}[カテゴリカル分布]
  \[
    \Omega=\{0,1,2,\dots,n\},\quad M=\left\{(\xi^1,\dots,\xi^n)\mid\xi^i>0, \sum_{i=1}^n\xi^i<1\right\}
  \]
  に対して、確率密度を
  \[
    p(x;\xi)=\begin{cases}
      \xi^x & (1\leq x \leq n)\\
      1-\sum_{i=1}^n\xi^i & (x=0)
    \end{cases}
  \]
  で定義すると、これは$n$次元可微分統計モデルである。
\end{example}
カテゴリカル分布は、一言でいえば歪みありの$n$面サイコロの分布です。
そう言ってしまえば、これはある意味最もシンプルな例ですが、現実世界での応用例は広いです。
LLMも基本的にはカテゴリカル分布で「次の単語」を予想しており、
\begin{center}
  「昔々あるところに」に続く単語は？
\end{center}
と言われれば、「おじいさん」「浦島太郎」あたりの確率がほかの単語よりも高くなります。

\subsection{Fisher情報量}

上記の具体例を座右に、Fisher情報量を定義しましょう。
\begin{definition}[Fisher情報量]
  $(\Omega,\mathcal{B},M,S)$を$n$次元可微分統計モデルとする。
  このとき、以下の期待値からなる行列
  \[
    g_{ij}(\xi):=E_\xi\left[\frac{\partial \log p(x;\xi)}{\partial \xi^i} \, \frac{\partial \log p(x;\xi)}{\partial \xi^j}\right]
  \]
  を\textbf{Fisher情報量}あるいは\textbf{Fisher情報行列}と呼ぶ。
\end{definition}

次の例で見るように、健全な確率分布においてはFisher情報量は正則です。
一般にこのことは成り立たないので、統計モデルのレベルで仮定する必要があります。

\begin{definition}[正則統計モデル]
  Fisher情報行列が正則であるような可微分統計モデルを、\textbf{正則統計モデル}と呼ぶ。
\end{definition}

\subsubsection{Fisher情報量の例}

\begin{example}[正規分布のFisher情報量]
  平均$\mu\in\mathbb{R}$、偏差$\sigma>0$の正規分布の場合
  \[
    \log p(x;\mu,\sigma)=-\frac12\log 2\pi - \log \sigma - \frac{(x-\mu)^2}{2\sigma^2}
  \]
  より
  \begin{align*}
    \frac{\partial \log p(x;\mu,\sigma)}{\partial \mu}&=\frac{x-\mu}{\sigma^2}\\
    \frac{\partial \log p(x;\mu,\sigma)}{\partial \sigma}&= - \frac1\sigma + \frac{(x-\mu)^2}{\sigma^3}
  \end{align*}
  期待値を計算すると、
  \begin{align*}
    g_{\mu\mu}&=\int_\mathbb{R}\frac{(x-\mu)^2}{\sigma^4}\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}dx=\frac1{\sigma^2}\\
    g_{\mu\sigma}=g_{\sigma\mu}&=\int_\mathbb{R}\frac{x-\mu}{\sigma^2}\left\{- \frac1\sigma + \frac{(x-\mu)^2}{\sigma^3}\right\}\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}dx=0\\
    g_{\sigma\sigma}&=\int_\mathbb{R}\left\{- \frac1\sigma + \frac{(x-\mu)^2}{\sigma^3}\right\}^2\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}dx=\frac{2}{\sigma^2}
  \end{align*}
  となっている。
  計量としてまとめると、
  \[
    ds^2=\frac{d\mu^2+2d\sigma^2}{\sigma^2}
  \]
  となり、これは上半平面$\{(\mu,\sigma)\mid\mu\in\mathbb{R}, \sigma>0\}$におけるPoincar\'e計量によく似た計量である。
\end{example}

\begin{example}[Poisson分布]
  \[
    \Omega=\{0,1,2,\dots\},\quad n=1,\quad M=\{\xi\mid\xi>0\}
  \]
  に対して確率分布の族
  \[
    p(x;\xi)=e^{-\xi}\frac{\xi^x}{x!}
  \]
  について考える。
  このとき、
  \[
    \log p(x;\xi)=- \xi + x\log\xi - \sum_{i=1}^x\log i
  \]
  のため、
  \[
    \frac{\partial \log p(x;\xi)}{\partial \xi}=-1+\frac{x}{\xi}
  \]
  となり、
  \[
    g_{\xi\xi}=E\left[\left(-1+\frac{X}{\xi}\right)^2\right]=\frac1{\xi^2} E[(X-\xi)^2]=\frac1\xi
  \]
  となる。計量の形式で書けば、
  \[
    ds^2=\frac{d\xi^2}{\xi}
  \]
  となり、$\theta:=2\sqrt{\xi}$と変数変換すれば
  \[
    ds^2=d\theta^2
  \]
  となり、平坦な直線と同一になる。
\end{example}

\begin{example}[有限集合の場合]
  \[
    \Omega=\{0,1,2,\dots,n\},\quad M=\left\{(\xi^1,\dots,\xi^n)\mid\xi^i>0, \sum_{i=1}^n\xi^i<1\right\}
  \]
  に対して、確率密度を
  \[
    p(x;\xi)=\begin{cases}
      \xi^x & (1\leq x \leq n)\\
      1-\sum_{i=1}^n\xi^i & (x=0)
    \end{cases}
  \]
  で定義する。
  このとき
  \[
    \log p(x;\xi)=\begin{cases}
      \log \xi^x & (1\leq x \leq n)\\
      \log \left(1-\sum_i\xi^i\right) & (x=0)
    \end{cases}
  \]
  となるから、
  \[
    \frac{\partial \log p(x;\xi)}{\partial \xi^i}=\begin{cases}
      \delta_{xi}\frac1{\xi^i} & (1\leq x \leq n)\\
      -\frac{1}{1-\sum_i \xi^i} & (x=0)
    \end{cases}
  \]
  であり、
  \begin{align*}
    g_{ij}(\xi)&=\sum_{x=0}^n\frac{\partial \log p(x;\xi)}{\partial \xi^i} \, \frac{\partial \log p(x;\xi)}{\partial \xi^j} \, p(x;\xi)\\
    &=\frac{1}{1-\sum_k \xi^k}+\sum_{x=1}^n\delta_{xi}\frac1{\xi^i}\delta_{xj}\frac1{\xi^j}\xi^x\\
    &=\frac{1}{1-\sum_k \xi^k}+\begin{cases}
      0 & (i\neq j)\\
      \frac{1}{\xi^i} & (i=j)
    \end{cases}
  \end{align*}
  まとめると、
  \[
    ds^2=\frac{(\sum_i d\xi^i)^2}{1-\sum_k \xi^k}+\sum_{j=1}^n\frac{(d\xi^j)^2}{\xi^j}
  \]

  この計量は半径が2の球面$S^n(2)\subset\mathbb{R}^{n+1}$の一部に一致する計量である。
  実際、$S^n(2)$
  \[
    (y^0)^2+\cdots+(y^n)^2=4
  \]
  の計量は、チャート
  \[
    \{(y^1,\dots,y^n)\mid {\textstyle 0<\sum_{i=1}^n(y^i)^2<4}\}\to S^n;(y^1,\dots,y^n)\mapsto\left(\sqrt{\textstyle 4-\sum_{k=1}^n (y^k)^2},y^1,\dots,y^n\right)
  \]
  において、
  \[
    ds^2=\sum_{i=0}^n(dy^i)^2=\frac{(\sum_i y^idy^i)^2}{4-\sum_{k=1}^n(y^k)^2}+\sum_{i=1}^n (dy^i)^2
  \]
  ゆえに
  \[
    y^i=2\sqrt{\xi^i}
  \]
  と変数変換すると、$dy^i=d\xi^i/\sqrt{\xi^i}$より
  \[
    ds^2=\frac{(\sum_i d\xi^i)^2}{1-\sum_k \xi^k}+\sum_{j=1}^n\frac{(d\xi^j)^2}{\xi^j}
  \]
  となることがわかる。
\end{example}

\subsection{Fisher情報量は計量}

二次形式を値に持つ関数が計量であることを示すには、次の二点を証明する必要があります。
\begin{itemize}
  \item 正定値であること
  \item 座標変換に対して共変的に変換されること
\end{itemize}

まずは正定値であることを証明しましょう。
記号が大変になってきたので
\begin{align*}
  \ell(x;\xi)&:=\log p(x;\xi)\\
  \partial_i&:=\frac{\partial}{\partial\xi^i}
\end{align*}
とおきましょう。
任意の$\xi\in M$と任意のベクトル$v^i$に対して、
\begin{align*}
  v^ig_{ij}(\xi)v^j&=v^iE_\xi[\partial_i\ell(x;\xi) \, \partial_j\ell(x;\xi)]v^j\\
  &=E_\xi[v^iv^j\partial_i\ell(x;\xi) \, \partial_j\ell(x;\xi)]\\
  &=E_\xi[(v^i\partial_i\ell(x;\xi))^2]\geq0
\end{align*}
ゆえに半正定値であることはわかりました。
あとは$v^ig_{ij}(\xi)v^j=0 \iff v^i=0$を証明しなければなりませんが、これは$g_{ij}(\xi)$が正則行列をなすと仮定したことから明らかです。
これで$g_{ij}(\xi)$が正定値であることがわかりました。

次に座標変換に対して共変的に変換されることを証明します。
もう一つの局所座標系$\eta^j$を考えると、$\xi$から$\eta$への座標変換は
\begin{align*}
  \frac{\partial \ell(x;\xi)}{\partial \xi^i}=\frac{\partial \eta^j(\xi)}{\partial \xi^i} \, \frac{\partial \ell(x;\xi(\eta))}{\partial \eta^j}
\end{align*}
と変換されます。
したがって
\begin{align*}
  g_{ij}(\xi)
  &=E_\xi\left[\frac{\partial \ell(x;\xi)}{\partial \xi^i} \, \frac{\partial \ell(x;\xi)}{\partial \xi^j}\right]\\
  &=E_{\xi(\eta)}\left[\frac{\partial \eta^k(\xi)}{\partial \xi^i} \, \frac{\partial \ell(x;\xi(\eta))}{\partial \eta^k} \, \frac{\partial \eta^l(\xi)}{\partial \xi^j} \, \frac{\partial \ell(x;\xi(\eta))}{\partial \eta^l}\right]\\
  &=\frac{\partial \eta^k(\xi)}{\partial \xi^i} \, \frac{\partial \eta^l(\xi)}{\partial \xi^j} \, E_{\xi(\eta)}\left[\frac{\partial \ell(x;\xi(\eta))}{\partial \eta^k} \, \frac{\partial \ell(x;\xi(\eta))}{\partial \eta^l}\right]\\
  &=\frac{\partial \eta^k(\xi)}{\partial \xi^i} \, \frac{\partial \eta^l(\xi)}{\partial \xi^j} \, g_{kl}(\xi(\eta))
\end{align*}
となり、座標変換に対して共変的に変換されることが確認できました。

\begin{theorem}
  $(\Omega,\mathcal{B},M,S)$を$n$次元可微分統計モデルとする。
  このとき、$(M,\{g_{ij}\})$はRiemann多様体である。
\end{theorem}



\section{$\alpha$接続}

統計学や深層学習の分野では、分布推定の研究の中でいくつかの「分布の近さ」と呼べる概念があります。
先に説明したFisher情報量は、Riemann計量を与えるという意味ではまさにその典型例です。
その中でも微分幾何学における接続とかかわりが深い\textbf{ダイバージェンス}について知らねばならないでしょう。

\subsection{ダイバージェンス}

ある統計モデルの二つの確率分布$p,q$がどれぐらい離れているかを測るものとして、統計学や深層学習の界隈では次のKLダイバージェンスというものが知られています。
\begin{definition}
  $(\Omega,\mathcal{B})$を可測空間、$p:\mathbb{R}\to\mathbb{R}$および$q:\mathbb{R}\to\mathbb{R}$をその確率分布とする。
  このとき
  \[
    D_{\rm KL}(p \| q):=\int_\mathbb{R}p(x)\log\frac{p(x)}{q(x)}dx
  \]
  を、$p$から$q$への\textbf{Kullback-Leiblerダイバージェンス}または\textbf{KLダイバージェンス}と呼ぶ。
\end{definition}
要するに
\[
  \log \frac{p(X)}{q(X)}
\]
の$p$による期待値です。
$p=q$ならば明らかに$D_{\rm KL}(p \| p)=0$ですし、一般の測度論の\textbf{Jensenの不等式}を凸関数$-\log$に適用すると
\begin{align*}
  &D_{\rm KL}(p \| q)=E_p\left[-\log \frac{q(X)}{p(X)}\right]\\
  \geq&-\log E_p\left[\frac{q(X)}{p(X)}\right]=-\log \int_\mathbb{R}p(x)\frac{q(x)}{p(x)}dx=-\log \int_\mathbb{R}q(x)dx=-\log 1=0
\end{align*}
となって常に0以上であることも分かります。

\subsection{ごく近い二つの確率分布}

$(\Omega,\mathcal{B},M,S)$を$n$次元可微分統計モデルとします。

% TODO: KLダイバージェンスからFisher計量や指数型接続がでてきそう。






\section{カウンターイグザンプルズ}

\subsection{微分不可能な統計モデル}

\subsection{正則でないFisher情報量をもつ統計モデル}



\section{もっとマニアックな話}

\subsection{現在標準的な正則統計モデルの定義}


